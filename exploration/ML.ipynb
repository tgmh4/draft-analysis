{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, json, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "Parameterize: pro/pubs, tournament ID/bracket, hero\n",
    "\n",
    "Output: [`match_ids`] in .json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Gathering Data: via Filter Criteria (Pubs) (TBC)\n",
    "\n",
    "scrape `match_id`s based on specific criteria. (bracket, hero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder to modify get_match_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Gathering Data: via Tournament ID (pro games)\n",
    "\n",
    "ref: https://api.opendota.com/api/leagues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Example: TI13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_ids_from_league(league_id: int, limit: int | None = None) -> list[int]:\n",
    "    \"\"\"\n",
    "    Fetch match_ids for a given league.\n",
    "\n",
    "    Args:\n",
    "        league_id (int): The league ID to query.\n",
    "        limit (int | None): Optional maximum number of match_ids to return.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: List of match IDs.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.opendota.com/api/leagues/{league_id}/matches\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    matches = resp.json()\n",
    "\n",
    "    match_ids = [m[\"match_id\"] for m in matches if \"match_id\" in m]\n",
    "\n",
    "    if limit is not None:\n",
    "        return match_ids[:limit]\n",
    "    return match_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "league_id=16935 # TI14 16899\n",
    "\n",
    "match_ids = get_match_ids_from_league(league_id)\n",
    "len(match_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Write `match_ids` to file to limit API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ti13_match_ids.json', 'w') as f:\n",
    "    json.dump(match_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "After this point, selected `match_ids` are used as keys for further queries for any specific ML task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "\n",
    "1. Store match data in raw format locally.\n",
    "\n",
    "2. Selectively extract match keys to be aggregated into .csv files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Stage 1: Store match data\n",
    "\n",
    "Given `match_id`, save entire .json file to disk.\n",
    "\n",
    "\n",
    "Parameterize: .json file of `match_ids`\n",
    "\n",
    "Output: `data/raw_matches`of individual matches. Store in this format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = \"data/raw_matches\"\n",
    "\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def fetch_and_save_match(match_id, overwrite=False, sleep_time=1):\n",
    "    \"\"\"\n",
    "    Fetch a single match JSON from OpenDota and save to disk.\n",
    "    Skip if already exists unless overwrite=True.\n",
    "    \"\"\"\n",
    "    ensure_dir(RAW_DIR)\n",
    "    file_path = os.path.join(RAW_DIR, f\"{match_id}.json\")\n",
    "\n",
    "    if os.path.exists(file_path) and not overwrite:\n",
    "        print(f\"‚úÖ Skipping {match_id}, already cached.\")\n",
    "        return file_path\n",
    "\n",
    "    url = f\"https://api.opendota.com/api/matches/{match_id}\"\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(resp.json(), f, indent=2)\n",
    "        print(f\"üíæ Saved match {match_id} ‚Üí {file_path}\")\n",
    "        return file_path\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Failed to fetch {match_id}: {resp.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "\n",
    "def scrape_from_match_list(match_ids, overwrite=False):\n",
    "    \"\"\"\n",
    "    Given a list of match_ids, fetch and save them locally.\n",
    "    \"\"\"\n",
    "    saved_files = []\n",
    "    for mid in match_ids:\n",
    "        path = fetch_and_save_match(mid, overwrite=overwrite)\n",
    "        if path:\n",
    "            saved_files.append(path)\n",
    "    return saved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = scrape_from_match_list(match_ids)\n",
    "files[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Stage 2: Extract match keys and aggregate into .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_to_csv(input_dir, output_dir, keys):\n",
    "    \"\"\"\n",
    "    Extract specific keys from JSON match files and save as separate CSVs.\n",
    "    \n",
    "    Each CSV will be titled `<key>.csv` (e.g. picks_bans.csv).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir : str\n",
    "        Folder with raw JSON files (one per match).\n",
    "    output_dir : str\n",
    "        Folder to save CSV outputs.\n",
    "    keys : list[str]\n",
    "        Keys inside match JSONs to extract (e.g. [\"picks_bans\", \"draft_timings\"]).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for key in keys:\n",
    "        all_rows = []\n",
    "        for fname in os.listdir(input_dir):\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            with open(os.path.join(input_dir, fname)) as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            match_id = data.get(\"match_id\")\n",
    "            if key in data and isinstance(data[key], list):\n",
    "                for row in data[key]:\n",
    "                    row[\"match_id\"] = match_id\n",
    "                    all_rows.append(row)\n",
    "\n",
    "        if all_rows:\n",
    "            df = pd.DataFrame(all_rows)\n",
    "            out_file = os.path.join(output_dir, f\"{key}.csv\")\n",
    "            df.to_csv(out_file, index=False)\n",
    "            print(f\"üíæ Saved {len(df)} rows ‚Üí {out_file}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No data found for key `{key}` in provided matches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_json_to_csv(input_dir='data/raw_matches/', \n",
    "                    output_dir='data/', \n",
    "                    keys=['picks_bans', 'draft_timings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Turn raw data into recognizable inputs for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_df = pd.read_csv('data/picks_bans.csv')\n",
    "dt_df = pd.read_csv('data/draft_timings.csv')\n",
    "pb_df.info(), dt_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "For simplicity, we will use only `picks_bans`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### `merge_csvs()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for future\n",
    "\n",
    "def merge_csvs(csv_paths, merge_keys=[\"match_id\", \"order\"]):\n",
    "    \"\"\"\n",
    "    Merge multiple CSVs on given keys (default: match_id + order).\n",
    "    Uses inner join to keep only rows where all files align.\n",
    "    \"\"\"\n",
    "    if not csv_paths:\n",
    "        raise ValueError(\"No CSV paths provided\")\n",
    "\n",
    "    # Load first file\n",
    "    merged_df = pd.read_csv(csv_paths[0])\n",
    "\n",
    "    # Iteratively merge the rest\n",
    "    for path in csv_paths[1:]:\n",
    "        df = pd.read_csv(path)\n",
    "        merged_df = merged_df.merge(df, on=merge_keys, how=\"inner\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "# csvs = [\"data/picks_bans.csv\", \"data/draft_timings.csv\"]\n",
    "# df_merged = merge_csvs(csvs)\n",
    "# display(df_merged.head())\n",
    "# print(f\"Final shape: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Cleaning, assigning target variable, splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure clean dtypes\n",
    "pb_df = pb_df.sort_values(by=[\"match_id\", \"order\"])\n",
    "pb_df[\"is_pick\"] = pb_df[\"is_pick\"].astype(int)\n",
    "pb_df[\"team\"] = pb_df[\"team\"].astype(int)\n",
    "\n",
    "# X: context features\n",
    "X = pb_df[[\"team\", \"is_pick\", \"order\"]]\n",
    "y = pb_df[\"hero_id\"]\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.head(), X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train a single model and return evaluation metrics.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_models(models_dict, X_train, y_train_enc, X_test, y_test_enc):\n",
    "    \"\"\"\n",
    "    Train and benchmark multiple models.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for name, model in models_dict.items():\n",
    "        print(f\"‚ö° Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_test, y_test)\n",
    "        metrics[\"model\"] = name\n",
    "        results.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(results).set_index(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, multi_class=\"multinomial\"),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"mlogloss\"),\n",
    "    \"LightGBM\": LGBMClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = benchmark_models(models_dict, X_train, y_train, X_test, y_test)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "As expected, the models could not detect underlying domain structure that dictates the choices of picks and bans.\n",
    "\n",
    "With baseline established, potential experiments include:\n",
    "\n",
    "**Baseline 2.0**\n",
    "\n",
    "‚Ä¢\tRerun but incorporate past TIs, Majors etc\n",
    "\n",
    "**Role prediction**\n",
    "\n",
    "‚Ä¢\tFeature engineering: normalize stats, add per-role averages\n",
    "\n",
    "‚Ä¢\tTrain models (role classification)\n",
    "\n",
    "**Hero prediction conditional on role**\n",
    "\n",
    "‚Ä¢\tUse predicted role as an input\n",
    "\n",
    "‚Ä¢\tReduce class imbalance\n",
    "\n",
    "**Sequential models**\n",
    "\n",
    "‚Ä¢\tReframe input as sequences (e.g., draft order, time-series stats)\n",
    "\n",
    "‚Ä¢\tTry RNN/LSTM/Transformer baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dota-dashboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
